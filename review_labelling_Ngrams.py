# -*- coding: utf-8 -*-
"""i210390_Vaneeza_A1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HgcmyjU9TkfOEYB8vmqmvA04ZJNtH0r
"""

import pandas as pd

df = pd.read_csv('imdb_dataset.csv')
df.head()
#print the first 5 rows

# data cleaning

def data_cleaning(df):
  #remove duplicates
  #df.drop_duplicates(inplace=True)
  #remove <br/>
  df['review'] = df['review'].str.replace('<br />', '')
  #remove null values
  df.dropna(inplace=True)
  #convert non-string to string
  df['review'] = df['review'].astype(str)
  #remove digits
  df['review'] = df['review'].str.replace('\d+', '')
  #fill missing values
  df['review'] = df['review'].fillna('').astype(str).str.lower()
  return df

df = data_cleaning(df)
print(df.head())

#pre-processing using nltk
import nltk as nl
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
#download punkt sentence tokenizer models
nl.download('punkt')
nl.download('stopwords')

def data_preprocessing(df):
  #convert to lower case
  df['review'] = df['review'].str.lower()

  #remove punctuation
  df['review'] = df['review'].str.replace('[^\w\s]','', regex=True)

  #tokenization
  df['review'] = df['review'].apply(lambda x: nl.word_tokenize(x))

  return df

df = data_preprocessing(df)
df.head()

#unigram model

def unigram_model():
  #count word frequency
  word_freq = {}
  for review in df['review']:
    for word in review:
      if word in word_freq:
        word_freq[word] += 1
      else:
        word_freq[word] = 1

  #sort word frequency
  sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

  #calculate probability distribution
  total_words = sum(word_freq.values())
  prob_dist = {word: freq/total_words for word, freq in word_freq.items()}

  #laplace smoothing
  k = 1
  smoothed_prob_dist = {word: (freq + k)/(total_words + k*len(word_freq)) for word, freq in word_freq.items()}

  return sorted_word_freq, prob_dist, smoothed_prob_dist

sorted_word_freq, prob_dist, smoothed_prob_dist = unigram_model()
#print(sorted_word_freq[:10])
#print(prob_dist)
print(smoothed_prob_dist)
#print the highest word in smoothed_prob_dist
print(max(smoothed_prob_dist, key=smoothed_prob_dist.get))

import random

def predict_next_word_unigram():
    # get user input (though this won't be used for actual unigram prediction)
    user_input = input("Enter a word or sentence: ")

    # use unigram model
    sorted_word_freq, prob_dist, smoothed_prob_dist = unigram_model()

    # randomly select the next word based on the unigram probabilities
    # not looking at user input context bcz unigram

    # choose the most frequest word on the probability distribution
    next_word = max(smoothed_prob_dist, key=smoothed_prob_dist.get)

    return next_word

print(predict_next_word_unigram())

def bigram_model():
  #generate bigrams
  bigrams = []
  for review in df['review']:
    for i in range(len(review)-1):
      bigrams.append((review[i], review[i+1]))

  #frequency of each bigram
  bigram_freq = {}
  for bigram in bigrams:
    if bigram in bigram_freq:
      bigram_freq[bigram] += 1
    else:
      bigram_freq[bigram] = 1

  #count unigram frequencies
  unigram_freq = {}
  for review in df['review']:
    for word in review:
      if word in unigram_freq:
        unigram_freq[word] += 1
      else:
        unigram_freq[word] = 1

  #calculate conditional probability (for bigrams)
  cond_prob = {}
  for bigram, freq in bigram_freq.items():
    word1, word2 = bigram
    cond_prob[bigram] = freq/unigram_freq[word1]

  #laplace smoothing
  k=1
  smoothed_cond_prob = {}
  for bigram, freq in bigram_freq.items():
    word1, word2 = bigram
    smoothed_cond_prob[bigram] = (freq + k)/(unigram_freq[word1] + k*len(unigram_freq))

  return bigram_freq, cond_prob, smoothed_cond_prob

#bigram_freq, cond_prob, smoothed_cond_prob = bigram_model()
#print(bigram_freq)
#print(cond_prob)
#print(smoothed_cond_prob)

#next word prediction using bigram
import random
def predict_next_word_bigram():
  #get user input
  user_input = input("Enter a word or sentence: ")
  user_input = user_input.split()[-1]

  #use bigram model
  bigram_freq, cond_prob, smoothed_cond_prob = bigram_model()

  #select the next word based on the input word
  next_word = None
  max_prob = 0
  for bigram, prob in smoothed_cond_prob.items():
    word1, word2 = bigram
    if word1 == user_input:
      if prob > max_prob:
        max_prob = prob
        next_word = word2

  return next_word

print(predict_next_word_bigram())

def trigram_model():
  #generate trigrams
  trigrams = []
  for review in df['review']:
    for i in range(len(review)-2):
      trigrams.append((review[i], review[i+1], review[i+2]))

  #frequency of each trigram
  trigram_freq = {}
  for trigram in trigrams:
    if trigram in trigram_freq:
      trigram_freq[trigram] += 1
    else:
      trigram_freq[trigram] = 1

  #count bigram frequencies
  bigram_freq = {}
  for review in df['review']:
    for i in range(len(review)-1):
      bigram = (review[i], review[i+1])
      if bigram in bigram_freq:
        bigram_freq[bigram] += 1
      else:
        bigram_freq[bigram] = 1

  #calculate conditional probability (for trigrams)
  cond_prob = {}
  for trigram, freq in trigram_freq.items():
    word1, word2, word3 = trigram
    cond_prob[trigram] = freq/bigram_freq[(word1, word2)]

  #laplace smoothing
  k=1
  smoothed_cond_prob = {}
  for trigram, freq in trigram_freq.items():
    word1, word2, word3 = trigram
    smoothed_cond_prob[trigram] = (freq + k)/(bigram_freq[(word1, word2)] + k*len(bigram_freq))

  return trigram_freq, cond_prob, smoothed_cond_prob

#trigram_freq, cond_prob, smoothed_cond_prob = trigram_model()
#print(trigram_freq)
#print(cond_prob)
#print(smoothed_cond_prob)

#next word prediction using trigram
def predict_next_word_trigram():
  #get user input
  user_input = input("Enter a word or sentence: ")
  user_input = user_input.split()[-2:]

  #use trigram model
  trigram_freq, cond_prob, smoothed_cond_prob = trigram_model()

  #select the next word based on the input word
  next_word = None
  max_prob = 0
  for trigram, prob in smoothed_cond_prob.items():
    word1, word2, word3 = trigram
    if word1 == user_input[0] and word2 == user_input[1]:
      if prob > max_prob:
        max_prob = prob
        next_word = word3

  return next_word

print(predict_next_word_trigram())

from collections import Counter

def compute_likelihoods(df, smoothed_prob_dist):
    likelihoods = {'positive': {}, 'negative': {}}

    for sentiment in ['positive', 'negative']:
        # to store all words for the current sentiment
        words = []

        # for each review in the class
        for review in df[df['sentiment'] == sentiment]['review']:
            words.extend(review)

        # count word frequencies in a class
        word_counter = Counter(words)
        word_count = sum(word_counter.values())  # Total word count

        # Calculate the likelihood for each word that appears in the Counter
        for word, word_freq in word_counter.items():
            # Laplace smoothing
            likelihoods[sentiment][word] = (word_freq + 1) / (word_count + len(smoothed_prob_dist))

    return likelihoods['positive'], likelihoods['negative']

#naive bayes classifier
def naive_bayes_classifier_unigram():
  #compute prior probabilites
  prior_prob = df['sentiment'].value_counts(normalize=True)
  positive_prior = prior_prob['positive']
  negative_prior = prior_prob['negative']

  #compute likelihood probabilities using unigram model
  sorted_word_freq, prob_dist, smoothed_prob_dist = unigram_model()
  positive_likelihoods, negative_likelihoods = compute_likelihoods(df, smoothed_prob_dist)

  return positive_prior, negative_prior, positive_likelihoods, negative_likelihoods

positive_prior, negative_prior, positive_likelihoods, negative_likelihoods = naive_bayes_classifier_unigram()

#prediction using naive bayes unigram classifier
import math
def classify_using_unigram(user_input):
  user_input = user_input.split()
  positive_prob = math.log(positive_prior)
  negative_prob = math.log(negative_prior)

  # Calculate the log probability for each word
  for word in user_input:
      if word in positive_likelihoods:
          positive_prob += math.log(positive_likelihoods[word])
      else:
          # Apply Laplace smoothing for unseen words
          positive_prob += math.log(1 / (sum(positive_likelihoods.values()) + len(positive_likelihoods)))

      if word in negative_likelihoods:
          negative_prob += math.log(negative_likelihoods[word])
      else:
          # Apply Laplace smoothing for unseen words
          negative_prob += math.log(1 / (sum(negative_likelihoods.values()) + len(negative_likelihoods)))

  if positive_prob > negative_prob:
      return "positive"
  else:
      return "negative"

user_input = input("Enter a review: ").lower()
print(classify_using_unigram(user_input))

def compute_bigram_likelihoods(df, smoothed_bigram_prob_dist):
    likelihoods = {'positive': {}, 'negative': {}}

    for sentiment in ['positive', 'negative']:
        bigrams = []

        for review in df[df['sentiment'] == sentiment]['review']:
            # Generate bigrams for the review
            bigrams.extend([(review[i], review[i+1]) for i in range(len(review)-1)])

        bigram_counter = Counter(bigrams)

        # Total bigram count for the class
        bigram_count = sum(bigram_counter.values())

        # calculate likelihood for each bigram in a class
        for bigram, count in bigram_counter.items():
            if bigram in smoothed_bigram_prob_dist:
                likelihoods[sentiment][bigram] = smoothed_bigram_prob_dist[bigram]
            else:
                # Laplace smoothing
                likelihoods[sentiment][bigram] = 1 / (bigram_count + len(smoothed_bigram_prob_dist))

    return likelihoods['positive'], likelihoods['negative']

def naive_bayes_classifier_bigram():
  #compute prior probabilites
  prior_prob = df['sentiment'].value_counts(normalize=True)
  positive_prior = prior_prob['positive']
  negative_prior = prior_prob['negative']

  #compute likelihood probabilities using bigram model
  bigram_freq, cond_prob, smoothed_cond_prob = bigram_model()
  positive_likelihoods, negative_likelihoods = compute_bigram_likelihoods(df, smoothed_cond_prob)
  return positive_prior, negative_prior, positive_likelihoods, negative_likelihoods

positive_prior_b, negative_prior_b, positive_likelihoods_b, negative_likelihoods_b = naive_bayes_classifier_bigram()

def classify_using_bigram(user_input):
  user_input = user_input.split()

  # bigrams from user input
  bigrams_input = [(user_input[i], user_input[i+1]) for i in range(len(user_input) - 1)]

  import math

  # prior probabilities
  positive_prob = math.log(positive_prior_b)
  negative_prob = math.log(negative_prior_b)

  # Calculate probability for each bigram
  for bigram in bigrams_input:
      # For positive class
      if bigram in positive_likelihoods_b:
          positive_prob += math.log(positive_likelihoods_b[bigram])
      else:
          # Laplace smoothing
          positive_prob += math.log(1 / (sum(positive_likelihoods_b.values()) + len(positive_likelihoods_b)))

      # For negative class
      if bigram in negative_likelihoods_b:
          negative_prob += math.log(negative_likelihoods_b[bigram])
      else:
          # Laplace smoothing
          negative_prob += math.log(1 / (sum(negative_likelihoods_b.values()) + len(negative_likelihoods_b)))

  # Compare probs
  if positive_prob > negative_prob:
      #print("The review is positive")
      return "positive"
  else:
      #print("The review is negative")
      return "negative"

user_input = input("Enter a review: ")
classify_using_bigram(user_input)

#trigram likelihoods
def compute_trigram_likelihoods(df, smoothed_trigram_prob_dist):
    likelihoods = {'positive': {}, 'negative': {}}

    for sentiment in ['positive', 'negative']:
        trigrams = []

        for review in df[df['sentiment'] == sentiment]['review']:
            # Generate trigrams for the review
            trigrams.extend([(review[i], review[i+1], review[i+2]) for i in range(len(review)-2)])

        trigram_counter = Counter(trigrams)

        # Total trigram count per class
        trigram_count = sum(trigram_counter.values())

        # calculate likelihood for each trigram in a class
        for trigram, count in trigram_counter.items():
            if trigram in smoothed_trigram_prob_dist:
                likelihoods[sentiment][trigram] = smoothed_trigram_prob_dist[trigram]
                # Laplace smoothing
            else:
                likelihoods[sentiment][trigram] = 1 / (trigram_count + len(smoothed_trigram_prob_dist))

    return likelihoods['positive'], likelihoods['negative']

def naive_bayes_classifier_trigram():
  #compute prior probabilites
  prior_prob = df['sentiment'].value_counts(normalize=True)
  positive_prior = prior_prob['positive']
  negative_prior = prior_prob['negative']

  #compute likelihood probabilities using trigram model
  trigram_freq, cond_prob, smoothed_cond_prob = trigram_model()
  positive_likelihoods, negative_likelihoods = compute_trigram_likelihoods(df, smoothed_cond_prob)
  return positive_prior, negative_prior, positive_likelihoods, negative_likelihoods

positive_prior_t, negative_prior_t, positive_likelihoods_t, negative_likelihoods_t = naive_bayes_classifier_trigram()

# Prediction using Naive Bayes trigram classifier
def classify_using_trigram(user_input):
  user_input = user_input.split()

  # trigrams from input
  trigrams_input = [(user_input[i], user_input[i+1], user_input[i+2]) for i in range(len(user_input) - 2)]

  import math

  # prior probabilities
  positive_prob = math.log(positive_prior_t)
  negative_prob = math.log(negative_prior_t)

  # Calculate probability for each trigram
  for trigram in trigrams_input:
      # For positive class
      if trigram in positive_likelihoods_t:
          positive_prob += math.log(positive_likelihoods_t[trigram])
          # Laplace smoothing
      else:
          positive_prob += math.log(1 / (sum(positive_likelihoods_t.values()) + len(positive_likelihoods_t)))

      # For negative class
      if trigram in negative_likelihoods_t:
          negative_prob += math.log(negative_likelihoods_t[trigram])
          # Laplace smoothing
      else:
          negative_prob += math.log(1 / (sum(negative_likelihoods_t.values()) + len(negative_likelihoods_t)))
  # Compare probs
  if positive_prob > negative_prob:
      #print("The review is positive")
      return "positive"
  else:
      #print("The review is negative")
      return "negative"

user_input = input("Enter a review: ")
classify_using_trigram(user_input)

#generate a review using the language model

# unigram review generation
import random

def generate_review_unigram(smoothed_prob_dist, max_words=20):
    review = []

    # Get the most frequent word (based on unigram probabilities)
    most_frequent_word = max(smoothed_prob_dist, key=smoothed_prob_dist.get)

    for _ in range(max_words):
        review.append(most_frequent_word)

    return ' '.join(review)

sorted_word_freq, prob_dist, smoothed_prob_dist = unigram_model()
generated_review = generate_review_unigram(smoothed_prob_dist, max_words=20)
print(generated_review)
print("Unigram Classifier:", classify_using_unigram(generated_review))
print("Bigram Classifier:", classify_using_bigram(generated_review))
print("Trigram Classifier:", classify_using_trigram(generated_review))

# bigram review generation
import random

def generate_review_bigram(bigram_model, unigram_model, seed_word=None, max_words=20):
    review = []

    if seed_word is None:
        seed_word = random.choices(list(unigram_model.keys()), weights=unigram_model.values(), k=1)[0]

    review.append(seed_word)

    # Generate words based on bigrams
    for _ in range(max_words - 1):
        last_word = review[-1]

        # Get possible bigrams that start with the last word
        next_word_candidates = {pair[1]: prob for pair, prob in bigram_model.items() if pair[0] == last_word}

        if next_word_candidates:
            # choose the next word based on bigram probabilities
            next_word = random.choices(list(next_word_candidates.keys()), weights=next_word_candidates.values(), k=1)[0]
        else:
            # use unigram model if no bigrams
            next_word = random.choices(list(unigram_model.keys()), weights=unigram_model.values(), k=1)[0]

        review.append(next_word)

    return ' '.join(review)

# use bigram model
bigram_freq, cond_prob, smoothed_cond_prob = bigram_model()
unigram_freq, prob_dist, smoothed_prob_dist_unigram = unigram_model()

generated_review_bigram = generate_review_bigram(smoothed_cond_prob, smoothed_prob_dist_unigram, seed_word="the", max_words=20)
print(generated_review_bigram)
print("Unigram Classifier:", classify_using_unigram(generated_review_bigram))
print("Bigram Classifier:", classify_using_bigram(generated_review_bigram))
print("Trigram Classifier:", classify_using_trigram(generated_review_bigram))

import random

def generate_review_trigram(trigram_model, bigram_model, unigram_model, seed_phrase=None, max_words=20):
    review = []

    # Start with a seed
    if seed_phrase is None:
        seed_word1 = random.choices(list(unigram_model.keys()), weights=unigram_model.values(), k=1)[0]
        seed_word2 = random.choices(list(unigram_model.keys()), weights=unigram_model.values(), k=1)[0]
    else:
        seed_word1, seed_word2 = seed_phrase.split()[:2]

    review.extend([seed_word1, seed_word2])

    # Generate words based on trigrams
    for _ in range(max_words - 2):
        last_bigram = (review[-2], review[-1])

        # Trigrams starting from prev bigram
        next_word_candidates = {trigram[2]: prob for trigram, prob in trigram_model.items() if trigram[:2] == last_bigram}

        if next_word_candidates:
            # next word based on trigram model
            next_word = random.choices(list(next_word_candidates.keys()), weights=next_word_candidates.values(), k=1)[0]
        else:
            # use bigram model if no trigrams
            bigram_candidates = {bigram[1]: prob for bigram, prob in bigram_model.items() if bigram[0] == last_bigram[1]}
            if bigram_candidates:
                next_word = random.choices(list(bigram_candidates.keys()), weights=bigram_candidates.values(), k=1)[0]
            else:
                # use unigram model if no bigrams
                next_word = random.choices(list(unigram_model.keys()), weights=unigram_model.values(), k=1)[0]

        review.append(next_word)

    return ' '.join(review)

# use trigram model
trigram_freq, cond_prob, smoothed_cond_prob = trigram_model()
unigram_freq, prob_dist, smoothed_prob_dist_unigram = unigram_model()
bigram_freq, cond_prob, smoothed_cond_prob_bigram = bigram_model()
generated_review_trigram = generate_review_trigram(smoothed_cond_prob, smoothed_cond_prob_bigram, smoothed_prob_dist_unigram, seed_phrase="the film", max_words=20)
print(generated_review_trigram)
print("Unigram Classifier:", classify_using_unigram(generated_review_trigram))
print("Bigram Classifier:", classify_using_bigram(generated_review_trigram))
print("Trigram Classifier:", classify_using_trigram(generated_review_trigram))

#evaluate the model
def calculate_metrics(y_true, y_pred):
    TP = TN = FP = FN = 0

    # populate confusion matrix
    for true, pred in zip(y_true, y_pred):
        if true == 'positive' and pred == 'positive':
            TP += 1
        elif true == 'negative' and pred == 'negative':
            TN += 1
        elif true == 'negative' and pred == 'positive':
            FP += 1
        elif true == 'positive' and pred == 'negative':
            FN += 1

    # Calculate Accuracy
    accuracy = (TP + TN) / (TP + TN + FP + FN)

    # Calculate Precision, Recall, and F1-Score
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1_score:.4f}")

    return accuracy, precision, recall, f1_score

#create test review dataset
review1 = "The movie was really good. I loved it."
review2 = "The movie was bad. I hated it."
review3 = "The movie was okay. It was okay."
review4 = "I was expecting it to be good but it turned out worse."
review5 = "The acting was great, the people were great, and the story was great."
review6 = "The acting was terrible, very cringy."

y_true = ['positive', 'negative', 'positive', 'negative', 'positive', 'negative']
y_pred_unigram = [classify_using_unigram(review1), classify_using_unigram(review2), classify_using_unigram(review3), classify_using_unigram(review4), classify_using_unigram(review5), classify_using_unigram(review6)]
y_pred_bigram = [classify_using_bigram(review1), classify_using_bigram(review2), classify_using_bigram(review3), classify_using_bigram(review4), classify_using_bigram(review5), classify_using_bigram(review6)]
y_pred_trigram = [classify_using_trigram(review1), classify_using_trigram(review2), classify_using_trigram(review3), classify_using_trigram(review4), classify_using_trigram(review5), classify_using_trigram(review6)]

#evaluation of unigram based naive bayesian classifier
print("Evaluation Results for Unigram Based Naive Bayesian Classifer")
accuracy_unigram, precision_unigram, recall_unigram, f1_score_unigram = calculate_metrics(y_true, y_pred_unigram)

print("_____________________________________________________________")
#evaluation of bigram based naive bayesian classifier
print("Evaluation Results for Bigram Based Naive Bayesian Classifer")
accuracy_bigram, precision_bigram, recall_bigram, f1_score_bigram = calculate_metrics(y_true, y_pred_bigram)

print("_____________________________________________________________")
#evaluation of trigram based naive bayesian classifier
print("Evaluation Results for Trigram Based Naive Bayesian Classifer")
accuracy_trigram, precision_trigram, recall_trigram, f1_score_trigram = calculate_metrics(y_true, y_pred_trigram)

